#                                                                      Apache-Kafka-Doc-1
> Apache Kafka is a distributed event store and stream-processing platform. It is an open-source system developed by the Apache Software Foundation written in Java and Scala. 
> The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.

> Origin of Kafka -:
-------------------
#    > Linkdiin was introduced this concept later linkdin announced as an opensource in 2011.
#    > Then apache software foundation was adopted this kafka services and they placed in apache incubelater. then they did some updations and release the apache kafka in 2012

> What exactly kafka means -:
-----------------------------
#   > Simply Apache Kafka it's a distributed streaming platform. we can call kafka as a 
#   > Messaging system     -: For example I sent one meassage to a person B but in that time the person  B mobile was in switchedoff then after sometime he is turn on his mobile then immidiately the msg will receive to his mobile... 
#                             So mean while where that msg was storing, it's storing on kafka(if the organization using kafka) that's why we can call it's an messaging system. it's like a queing and pub-subcribe method.

#   > Storage system       -:  Why we calling storage system bcs for ex in the above example the message store in topic nah, by default kafka stores 7 days data. it keep store the data in the topic for one week. so we can read and we can restore that message. 
#                              By default It stores messages for one-week for that we can call it as it's an messaging system.

#   > Streaming processing -: It'll processing a critical data and we can modify the data, transform the data, and we can analyse the data. 
#                             This  Stream-processing is mostly uses in Risk management, Market analysis, Monitoring and network activities.

> Where we can use kafka -:
---------------------------
   > while we transfer the data in that time we use Kafka. It'll let us to publish and subscribe the stream of records.(like a meassage que system)
 
> Advantages of using kafka -:
------------------------------
   > It's scalable    > Secure   > Speed/Fast  > Reliable > Durable > stream-processing comapares to other alternatives. 
   > For ex if we have facing any system issue normally we lose the data but if we use the kafka we don't lose that data bcs kafka will store that data/records.
   > Another benifite kafka has a stream-process feature with help of this stream-processing feature we can process our records.
   > process in the sense we can transform (or) modify (or)  we can analyse those records. 
   > records are ntg but an our data (RECORDS = DATA). and in that data it'll record key, value, timestamp. 

> Compitators of Kafka -:
------------------------
#  We have some alternatives for kafa like 
#      > AMQP(Advanced messaging que protocal) -: It's also an open-source.
#      > RABITMQ                -: It's also an open-source, and it's an asynchroness messaging system.
#      > JMS                    -: Java messaging system. 
#      > Amazon kinesis         -: It has a highly capable and a scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds.
#      > Active MQ              -: It's also an apache service written in java and it's also provide HA, Scalable, high Performance, and speed.
#      > IBM MQ                 -: It's a piad version From IBM.
#      > MS MQ                  -: It's also a paid version form Microsoft.
# Depends on our strategy pattern we can use anyone one on these.

> Key-Concepts in kafka -:
--------------------------
#   > Records                   -:   Records are ntg but a collection of our data it'll record our data in key, value, timestamp 
   
#   > Topics                    -:   Topic is ntg but a particular stream of data (simply it's a collection of records, all records are store in one topic).bydefault it'll stores topic data for 7 days.
#                                    If we wanna change that days limit we can change it from to your desired days based on our requirement.

#   > Partitions                -:   A partition is the smallest storage unit that holds a subset of records owned by a topic.every partion is independent itself.

#   > OFFSets                   -:   Each message with in the partitions gets an incremental id(every message stores in partition with incremental id that id we call as offset).
#                                    incrimental means increasing(0,1,2,3,4,5.....n). for example i write a message in partion-0, offset 4 but i wanna change that message in that offeset. but it's not 
#                                    possible to change the data in offsets once we write the data that's why we call it as an immutable.

#   > Broker                    -:   Broker is ntg but an one server. like if you have one server on top of it we install a kafka and we configure broker with kafka. In kafka every broker does identify
#                                    with number like Broker0, Broker1, Broker2...so on. Every broker has ID the ID should be a Number. For example in a single cluster i have a total 5 brokers,
#                                    If i connect to one broker it'll connect to all  brokers. means if i connect to one broker it'll connect to entaire cluster.
#                                    There is no limit to create a brokers in a cluster based on our cluster resources and our requirements we can create n.number of brokers. 

#   > Single machine Kafka      -: 

#   > Cluster                   -:   Cluster means it's a Collection of kafka brokers. Means actually collection of kafka servers has a collection of kafka brokers. These brokers will maintain the data.

#  > For example if we have 3 kafka servers in my cluster so if one server is goes down it'll maintain a data in another two servers.
#  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Deep Dive of each component -:
=============================

  > Events -: 
  -----------
      > An event represents a fact that happened in the past. Events are immutable and never stay in one place. They always travel from one system to another system, carrying the state changes that happened.

  > Streams -:
    ---------
       > An event stream represents related events in motion.

   > Topics -:
    ----------
      > When an event stream enters Kafka, it is persisted as a topic. In Kafka’s universe, a topic is a materialized event stream. In other words, a topic is a stream at rest.
      > Topic groups related events together and durably stores them. The closest analogy for a Kafka topic is a table in a database or folder in a file system.
      > Topics are the central concept in Kafka that decouples producers and consumers. A consumer pulls messages off of a Kafka topic while producers push messages into a Kafka topic.
      > A topic can have many producers and many consumers.
      > Topic it's ntg but a particular stream of data. simply we can call it as a collection of records and all records we store in an one topic.(Where we can store our messages we can call a topic).
      > Every Topic we can identifies by it's Name. we can create number of topics on our own.
      > Every Topic has unique name, in kafka naming convention is very important.
      > When we create a topic we need to mention/provide How many partitions we required.
	   
   > Partitions -:
   ---------------
       > Kafka’s topics are divided into several partitions. While the topic is a logical concept in Kafka, a partition is the smallest storage unit that holds a subset of records owned by a topic. 
       > Each partition is a single log file where records are written to it in an append-only fashion.
       > When talking about the content inside a partition, we will use the terms record and message interchangeably.
       > We need to mention required partion count while we create a topic, and we can create n.num of partitions but partition does start with 0.(Partetion-0, partition-1, partition-2).
 
   > Offsets and the ordering of messages -:
   -----------------------------------------
           > The records in the partitions are each assigned a sequential identifier called the offset, which is unique for each record within the partition.
	   > The offset is an incremental and immutable number, maintained by Kafka. When a record is written to a partition, it is appended to the end of the log, assigning the next sequential offset.
	   > Offsets are particularly useful for consumers when reading records from a partition. We’ll come to that at a later point.
	   > The figure below shows a topic with three partitions. Records are being appended to the end of each one. Although the messages within a partition are ordered, messages across a topic are not guaranteed to be ordered.
       
    > Partitions are the way that Kafka provides scalability -:
      ---------------------------------------------------------
       > A Kafka cluster is made of one or more servers. In the Kafka universe, they are called Brokers. Each broker holds a subset of records that belongs to the entire cluster.
       > Kafka distributes the partitions of a particular topic across multiple brokers. By doing so, we’ll get the following benefits.
       > If we are to put all partitions of a topic in a single broker, the scalability of that topic will be constrained by the broker’s IO throughput. 
       > A topic will never get bigger than the biggest machine in the cluster. By spreading partitions across multiple brokers, a single topic can be scaled horizontally to provide performance far beyond a single broker’s ability.
       > A single topic can be consumed by multiple consumers in parallel. Serving all partitions from a single broker limits the number of consumers it can support. Partitions on multiple brokers enable more consumers.
       > Multiple instances of the same consumer can connect to partitions on different brokers, allowing very high message processing throughput. 
       > Each consumer instance will be served by one partition, ensuring that each record has a clear processing owner.
	   
	> Partitions are the way that Kafka provides redundancy -:
	----------------------------------------------------------
       > Kafka keeps more than one copy of the same partition across multiple brokers. This redundant copy is called a replica. If a broker fails, Kafka can still serve consumers with the replicas of partitions that failed broker owned.
       > Partition replication is complex, and it deserves its own post. Next time maybe?
    
	> Writing records to partitions -:
	----------------------------------
       > How does a producer decide to which partition a record should go? There are three ways a producer can rule on that
	   
	>  Using a partition key to specify the partition -:
	----------------------------------------------------
           > A producer can use a partition key to direct messages to a specific partition. A partition key can be any value that can be derived from the application context. A unique device ID or a user ID will make a good partition key.
           > By default, the partition key is passed through a hashing function, which creates the partition assignment. That assures that all records produced with the same key will arrive at the same partition. 
           > Specifying a partition key enables keeping related events together in the same partition and in the exact order in which they were sent.
	
	# Diagram referance (https://medium.com/event-driven-utopia/understanding-kafka-topic-partitions-ae40f80552e8)
    
	> Key based partition assignment can lead to broker skew if keys aren’t well distributed.
        > For example, when customer ID is used as the partition key, and one customer generates 90% of traffic, then one partition will be getting 90% of the traffic most of the time. 
	> On small topics, this is negligible, on larger ones, it can sometime take a broker down.
        > When choosing a partition key, ensure that they are well distributed.
    
	> Allowing Kafka to decide the partition -:
	--------------------------------------------
	   > If a producer doesn’t specify a partition key when producing a record, Kafka will use a round-robin partition assignment. Those records will be written evenly across all partitions of a particular topic.
           > However, if no partition key is used, the ordering of records can not be guaranteed within a given partition.
           > The key takeaway is to use a partition key to put related events together in the same partition in the exact order in which they were sent.
      
      > Writing a custom partitioner -:
      -------------------------------
        > In some situations, a producer can use its own partitioner implementation that uses other business rules to do the partition assignment.

      > Reading records from partitions -:
        ----------------------------------
        > Unlike the other pub/sub implementations, Kafka doesn’t push messages to consumers. Instead, consumers have to pull messages off Kafka topic partitions. 
	> A consumer connects to a partition in a broker, reads the messages in the order in which they were written.
        > The offset of a message works as a consumer side cursor at this point. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. 
	> After reading a message, the consumer advances its cursor to the next offset in the partition and continues. Advancing and remembering the last read offset within a partition is the responsibility of the consumer. Kafka has nothing to do with it.
        > By remembering the offset of the last consumed message for each partition, a consumer can join a partition at the point in time they choose and resume from there. That is particularly useful for a consumer to resume reading after recovering from a crash.
    
	# Diagram referance 
	 
        > A partition can be consumed by one or more consumers, each reading at different offsets.
        > Kafka has the concept of consumer groups where several consumers are grouped to consume a given topic. Consumers in the same consumer group are assigned the same group-id value.
        > The consumer group concept ensures that a message is only ever read by a single consumer in the group.

        > When a consumer group consumes the partitions of a topic, Kafka makes sure that each partition is consumed by exactly one consumer in the group.
	> The following figure shows the above relationship.
      
	 # Diagram referance 
     
	> Consumer groups enable consumers to parallelize and process messages at very high throughputs. However, the maximum parallelism of a group will be equal to the number of partitions of that topic.
        > For example, if you have N + 1 consumers for a topic with N partitions, then the first N consumers will be assigned a partition, and the remaining consumer will be idle, unless one of the N consumers fails, then the waiting consumer will be assigned its partition. 
	> This is a good strategy to implement a hot failover.
	 
	 # Diagram referance  
      
	 > The key takeaway is that number of consumers don’t govern the degree of parallelism of a topic. It’s the number of partitions.
  # NOTE -:
  # ======
        # >  ALL DIAGRAM STEPs REFERENCE YOU CAN VISIT (https://medium.com/event-driven-utopia/understanding-kafka-topic-partitions-ae40f80552e8)

# How to create a Topic in Brokers -:
===================================
